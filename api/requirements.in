# Web framework
fastapi
uvicorn

# Data handling
pydantic
pydantic-settings

# LLM and RAG
langchain-core
langchain-chroma
chromadb
langchain-text-splitters

# Qdrant + LlamaIndex for hybrid search
qdrant-client>=1.12.0
llama-index-core>=0.10.0
llama-index-vector-stores-qdrant>=0.2.0
llama-index-embeddings-openai>=0.1.0

# ColBERT reranking via RAGatouille
ragatouille>=0.0.8

# RAGAS evaluation framework
ragas>=0.1.0
datasets>=2.14.0

# NLI and ML models (for confidence scoring via DeBERTa)
# NOTE: torch is installed separately as CPU-only in Dockerfile/CI
# to avoid ~5GB of unnecessary NVIDIA CUDA libraries
transformers>=4.35.0
# torch - installed from https://download.pytorch.org/whl/cpu index

# Matrix integration
matrix-nio>=0.21.0

# LLM providers
# AISuite for unified LLM interface with MCP support
aisuite[mcp]>=0.1.14
# LiteLLM for vendor-portable embeddings (100+ providers)
litellm>=1.30.0
# MCP SDK for building Model Context Protocol servers
mcp[cli]>=1.0.0
# Pin to 1.x until we can test compatibility with 2.0.0 breaking changes
# 2.0.0 changes Response tool-call output schema (breaking change)
openai<2.0.0
# Keep langchain-openai for embeddings only (not for LLM)
langchain-openai
# Note: httpx is pinned to 0.27.2 in requirements.txt (aisuite requires httpx<0.28)

# Web scraping and parsing
beautifulsoup4
requests
urllib3>=2.6.0  # Security fix for CVE-2025-66418, CVE-2025-66471
mwclient

# Data processing
pandas
tqdm
aiohttp
mwxml
mwparserfromhell

# Resilience and caching (for Bisq MCP integration)
cachetools
pybreaker
tenacity
httpx  # async HTTP client with connection pooling
nest-asyncio  # Allow nested event loops for MCP tool execution

# Monitoring
psutil
prometheus-client
python-json-logger
prometheus-fastapi-instrumentator

# Testing and development
pytest
pytest-asyncio
black
isort
mypy
flake8
pip-tools
pre-commit

# Added for cross-process file locking
portalocker

# Data science libs
matplotlib
scikit-learn
pillow>=11.3.0
