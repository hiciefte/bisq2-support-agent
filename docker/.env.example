# Docker Environment Configuration
# This file is used when running the application with Docker Compose
# Values here are used by all Docker services unless specifically overridden
# ----------------------------------------------------------------------------

# =============================================================================
# OpenAI API Configuration
# =============================================================================
# OpenAI API key (required for LLM via AISuite)
OPENAI_API_KEY=
# Model ID with provider prefix for AISuite (format: "provider:model")
# Examples: openai:gpt-4o-mini, openai:gpt-4o, anthropic:claude-3-5-sonnet
OPENAI_MODEL=openai:gpt-4o-mini
MAX_TOKENS=4096
# Temperature for LLM responses (0.0-2.0, default: 0.7)
LLM_TEMPERATURE=0.7

# =============================================================================
# Embedding Provider Configuration (LiteLLM Multi-Provider)
# =============================================================================
# Embedding provider selection (openai, cohere, voyage, ollama)
# Default: openai (uses OPENAI_API_KEY above)
EMBEDDING_PROVIDER=openai
# Embedding model name (without provider prefix)
# OpenAI: text-embedding-3-small, text-embedding-3-large
# Cohere: embed-english-v3.0, embed-multilingual-v3.0
# Voyage: voyage-2, voyage-code-2
EMBEDDING_MODEL=text-embedding-3-small
# Optional: Embedding dimensions (model-dependent, can be omitted)
# OpenAI text-embedding-3-small supports 512 or 1536 dimensions
EMBEDDING_DIMENSIONS=
# API keys for alternative providers (only required if not using OpenAI)
COHERE_API_KEY=
VOYAGE_API_KEY=

# =============================================================================
# API Configuration
# =============================================================================
# Admin API key for protected endpoints
ADMIN_API_KEY=dev_admin_key
# Cookie security setting - set to false for HTTP, true for HTTPS
COOKIE_SECURE=false
# Debug mode - note: can be overridden in docker-compose files based on environment
DEBUG=false

# =============================================================================
# CORS Configuration
# =============================================================================
# CORS_ORIGINS - comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# =============================================================================
# Data Paths Configuration
# =============================================================================
# Directory where application data is stored (must match volume mount in docker-compose.yml)
DATA_DIR=/data

# =============================================================================
# Bisq Integration Configuration
# =============================================================================
# Bisq API URL for fetching support chat data
# Use Docker network hostname in production: http://bisq2-api:8090
# Use localhost for local development outside Docker: http://localhost:8090
BISQ_API_URL=http://bisq2-api:8090

# Support agent nicknames (comma-separated list of official support staff)
# Required for FAQ extraction from support chats
# Example: SUPPORT_AGENT_NICKNAMES=suddenwhipvapor,strayorigin,toruk-makto
# If not configured, no messages will be marked as support messages
SUPPORT_AGENT_NICKNAMES=suddenwhipvapor

# =============================================================================
# Bisq 2 MCP Integration Configuration (Live Data)
# =============================================================================
# Enable live Bisq 2 data integration for RAG queries
# When enabled, the system can answer questions about:
#   - Current market prices (BTC/USD, BTC/EUR, etc.)
#   - Live offerbook data (buy/sell offers)
#   - User reputation scores and profile information
# Default: false (disabled - requires bisq2-api service running)
ENABLE_BISQ_MCP_INTEGRATION=false

# MCP HTTP server URL for AISuite Native MCP client
# Default: http://localhost:8000/mcp (internal use only)
MCP_HTTP_URL=http://localhost:8000/mcp

# Bisq API request timeout in seconds (1-30)
# Lower values = faster failure detection, higher values = more reliability
# Default: 5 seconds
BISQ_API_TIMEOUT=5

# Cache TTL (Time-To-Live) in seconds for different data types
# Market prices cache (10-600 seconds, default: 120 = 2 minutes)
BISQ_CACHE_TTL_PRICES=120
# Offerbook data cache (5-300 seconds, default: 30 seconds)
BISQ_CACHE_TTL_OFFERS=30
# Reputation data cache (60-3600 seconds, default: 300 = 5 minutes)
BISQ_CACHE_TTL_REPUTATION=300

# =============================================================================
# Privacy and Security Settings
# =============================================================================
# Data retention period in days (GDPR compliance)
DATA_RETENTION_DAYS=30
# Enable privacy-preserving features (recommended for production)
ENABLE_PRIVACY_MODE=true
# Enable PII detection in logs
PII_DETECTION_ENABLED=true
# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# =============================================================================
# Port Configuration
# =============================================================================
# Direct port exposure (for direct access to services if needed)
EXPOSE_API_PORT=8000
EXPOSE_PROMETHEUS_PORT=9090
EXPOSE_GRAFANA_PORT=3001

# =============================================================================
# Monitoring Configuration
# =============================================================================
# Monitoring credentials
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=securepassword
PROMETHEUS_BASIC_AUTH_USERNAME=admin
PROMETHEUS_BASIC_AUTH_PASSWORD=prometheuspassword

# Token cost tracking (for Prometheus metrics)
# These values should match your configured OPENAI_MODEL pricing
# Default values are for GPT-4o-mini (as of 2024):
#   - Input: $0.15 per 1M tokens = $0.00000015 per token
#   - Output: $0.60 per 1M tokens = $0.0000006 per token
# For other models, see: https://openai.com/api/pricing/
OPENAI_INPUT_COST_PER_TOKEN=0.00000015
OPENAI_OUTPUT_COST_PER_TOKEN=0.0000006

# External health monitoring (Healthchecks.io)
# Sign up at https://healthchecks.io and create a check with:
#   - Schedule: Every 15 minutes
#   - Grace Time: 5 minutes
# The health check script will ping this URL if all RAG metrics are healthy
HEALTHCHECK_URL=https://hc-ping.com/YOUR-UUID-HERE

# =============================================================================
# Matrix Integration Configuration
# =============================================================================
# Shared Matrix auth (used by sync + alerts unless overridden)
# Examples: https://matrix.org, https://matrix.bisq.network
MATRIX_HOMESERVER_URL=https://matrix.org
MATRIX_USER=@bisq-support:matrix.org
# Preferred for automatic session management
MATRIX_PASSWORD=your_matrix_password_here
# Matrix sync lane (training ingestion)
# Explicit channel flag used by channel bootstrapper
MATRIX_SYNC_ENABLED=false
# Preferred key for monitored support rooms
MATRIX_SYNC_ROOMS=!YourRoomId:matrix.org
# Preferred key for sync session persistence
MATRIX_SYNC_SESSION_FILE=/data/matrix_session.json

# Matrix alert lane (Alertmanager notifications)
# Keep separate from support rooms (private ops room recommended)
MATRIX_ALERT_ROOM=
MATRIX_ALERT_SESSION_FILE=/data/matrix_alert_session.json

# =============================================================================
# LLM Extraction Configuration (Full LLM Solution)
# =============================================================================
# Enable full LLM-based question extraction from conversations
# When enabled, replaces pattern-based classification with LLM analysis
# Expected cost: $7-8/month at ~300 questions/month (using gpt-4o-mini)
ENABLE_LLM_EXTRACTION=false

# LLM model for question extraction (format: "provider:model")
# Recommended: openai:gpt-4o-mini for cost efficiency
# Alternative: openai:gpt-4o for higher accuracy (more expensive)
LLM_EXTRACTION_MODEL=openai:gpt-4o-mini

# Number of conversations to process in parallel batches
# Higher values = faster processing but more API rate limit pressure
# Default: 10 (balances speed and cost)
LLM_EXTRACTION_BATCH_SIZE=10

# Cache time-to-live in seconds (GDPR-friendly)
# Cached extraction results expire after this duration
# Default: 3600 (1 hour)
LLM_EXTRACTION_CACHE_TTL=3600

# Maximum tokens per conversation before truncation
# Prevents excessive API costs for very long conversations
# Default: 4000 (~3000 words)
LLM_EXTRACTION_MAX_TOKENS=4000

# LLM temperature for extraction (0.0-2.0)
# 0.0 = deterministic (recommended for consistent extraction)
# Higher values = more creative but less predictable
LLM_EXTRACTION_TEMPERATURE=0.0

# =============================================================================
# Auto-Training Pipeline Configuration
# =============================================================================
# Enable automatic training pipeline for staff answer ingestion
# When enabled, the system automatically extracts FAQs from trusted staff answers
# Default: false (disabled - enable only in production with trusted staff)
AUTO_TRAINING_ENABLED=false

# Comma-separated list of trusted staff full Matrix IDs
# Only answers from these users will be used for auto-training
# Format: @username:homeserver (e.g., @support:matrix.bisq.network)
TRUSTED_STAFF_IDS=@mwithm:matrix.bisq.network,@pazza83:matrix.bisq.network,@suddenwhipvapor:matrix.bisq.network,@strayorigin:matrix.bisq.network,@darawhelan:matrix.bisq.network,@luis3672:matrix.bisq.network

# API Settings
NEXT_PUBLIC_PROJECT_NAME="Bisq 2 Support Agent"
