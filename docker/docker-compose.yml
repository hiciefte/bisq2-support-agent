services:
  # Nginx reverse proxy for web frontend only
  nginx:
    image: nginx:alpine
    restart: unless-stopped
    environment:
      - API_URL=${API_URL:-/api} # For CSP configuration
    volumes:
      # Mount production config (with security headers enabled)
      - ./nginx/conf.d/default.prod.conf:/etc/nginx/conf.d/default.conf:ro
      # Mount other config files and snippets
      - ./nginx/conf.d/snippets:/etc/nginx/conf.d/snippets:ro
      - ./nginx/conf.d/tor-support.conf.template:/etc/nginx/conf.d/tor-support.conf.template:ro
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./logs/nginx:/var/log/nginx
      - ./nginx/error_pages:/usr/share/nginx/html/error_pages:ro
      # Mount maintenance page
      - ./maintenance/maintenance.html:/usr/share/nginx/html/maintenance.html:ro
    ports:
      # Bind to all interfaces for clearnet and Tor access
      - "0.0.0.0:80:80"
    depends_on:
      web:
        condition: service_healthy
      api:
        condition: service_healthy
    networks:
      - bisq-support-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:80" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    # Security hardening
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETGID
      - SETUID
      - NET_BIND_SERVICE
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /var/run
      - /var/cache/nginx
      - /tmp

  # Bisq2 API service
  bisq2-api:
    build:
      context: ./bisq2-api
      dockerfile: Dockerfile
      args:
        # Optional: uncomment and set if you need a different branch/repo
        # BISQ2_BRANCH: add-support-api
        # BISQ2_REPO_URL: https://github.com/hiciefte/bisq2.git
        APP_UID: 1001 # Match host user UID if needed for volume permissions
        APP_GID: 1001 # Match host user GID if needed for volume permissions
    restart: always
    volumes:
      - bisq2-data:/opt/bisq2/data
    environment:
      - BISQ_DATA_DIR=/opt/bisq2/data
      - JAVA_OPTS=-Xmx1g
      - BISQ_USER_RATE_LIMIT_ENABLED=${BISQ_USER_RATE_LIMIT_ENABLED:-true}
    expose:
      - "8090"
    networks:
      - bisq-support-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8090/api/v1/support/export" ]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    cap_add:
      - NET_ADMIN
    security_opt:
      - seccomp:unconfined

  # API backend
  api:
    build:
      context: ..
      dockerfile: docker/api/Dockerfile
      args:
        - BUILD_ENV=production
        - APP_UID=${APP_UID:-1001}
        - APP_GID=${APP_GID:-1001}
    restart: unless-stopped
    volumes:
      - ../api/app:/app/app # Mount the app directory for development
      - ../api/data:/data # Mount the data directory
      # Named volume disabled: causes permission issues in local dev (root:root ownership)
      # - feedback-data:/data/feedback # Named volume for persistent feedback storage
    environment:
      # Only Docker-specific overrides here
      - ENVIRONMENT=production # Environment-specific setting
      # OpenAI API configuration (used by AISuite for LLM and embeddings)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL}
      - OPENAI_EMBEDDING_MODEL=${OPENAI_EMBEDDING_MODEL}
      - MAX_TOKENS=${MAX_TOKENS}
      # Admin and security configuration
      - ADMIN_API_KEY=${ADMIN_API_KEY}
      - COOKIE_SECURE=${COOKIE_SECURE}
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
      - DATA_DIR=${DATA_DIR:-/data}
      - BISQ_API_URL=${BISQ_API_URL:-http://bisq2-api:8090} # URL for the containerized Bisq2 API service
      - BISQ2_CHANNEL_ENABLED=${BISQ2_CHANNEL_ENABLED:-false}
      - BISQ_API_AUTH_ENABLED=${BISQ_API_AUTH_ENABLED:-false}
      - BISQ_API_CLIENT_ID=${BISQ_API_CLIENT_ID:-}
      - BISQ_API_CLIENT_SECRET=${BISQ_API_CLIENT_SECRET:-}
      - BISQ_API_SESSION_ID=${BISQ_API_SESSION_ID:-}
      - BISQ_API_PAIRING_CODE_ID=${BISQ_API_PAIRING_CODE_ID:-}
      - BISQ_API_PAIRING_QR_FILE=${BISQ_API_PAIRING_QR_FILE:-}
      - BISQ_API_PAIRING_CLIENT_NAME=${BISQ_API_PAIRING_CLIENT_NAME:-bisq-support-agent}
      - BISQ_API_AUTH_STATE_FILE=${BISQ_API_AUTH_STATE_FILE:-bisq_api_auth.json}
      - ENABLE_BISQ_MCP_INTEGRATION=${ENABLE_BISQ_MCP_INTEGRATION:-false} # Enable Bisq 2 MCP integration for live data
      # Tor hidden service configuration
      - TOR_HIDDEN_SERVICE=${TOR_HIDDEN_SERVICE}
      # Privacy and security settings
      - DATA_RETENTION_DAYS=${DATA_RETENTION_DAYS:-30}
      - SUPPORT_AGENT_NICKNAMES=${SUPPORT_AGENT_NICKNAMES}
      - ENABLE_PRIVACY_MODE=${ENABLE_PRIVACY_MODE:-true}
      - PII_DETECTION_ENABLED=${PII_DETECTION_ENABLED:-true}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Matrix integration configuration (sync lane + alert lane)
      - MATRIX_HOMESERVER_URL=${MATRIX_HOMESERVER_URL:-}
      - MATRIX_SYNC_USER=${MATRIX_SYNC_USER:-}
      - MATRIX_SYNC_PASSWORD=${MATRIX_SYNC_PASSWORD:-}
      - MATRIX_ALERT_USER=${MATRIX_ALERT_USER:-}
      - MATRIX_ALERT_PASSWORD=${MATRIX_ALERT_PASSWORD:-}
      - MATRIX_SYNC_ENABLED=${MATRIX_SYNC_ENABLED:-false}
      - MATRIX_SYNC_ROOMS=${MATRIX_SYNC_ROOMS:-}
      - MATRIX_SYNC_SESSION_FILE=${MATRIX_SYNC_SESSION_FILE:-}
      - MATRIX_ALERT_ROOM=${MATRIX_ALERT_ROOM:-}
      - MATRIX_ALERT_SESSION_FILE=${MATRIX_ALERT_SESSION_FILE:-}
      - TRUSTED_STAFF_IDS=${TRUSTED_STAFF_IDS:-}
      - LLM_CLASSIFICATION_CACHE_SIZE=${LLM_CLASSIFICATION_CACHE_SIZE:-1000}
      - LLM_CLASSIFICATION_CACHE_TTL_HOURS=${LLM_CLASSIFICATION_CACHE_TTL_HOURS:-1}
      - LLM_CLASSIFICATION_MAX_CONCURRENT=${LLM_CLASSIFICATION_MAX_CONCURRENT:-2}
      - LLM_CLASSIFICATION_TEMPERATURE=${LLM_CLASSIFICATION_TEMPERATURE:-0.2}
      - LLM_CLASSIFICATION_RATE_LIMIT_REQUESTS=${LLM_CLASSIFICATION_RATE_LIMIT_REQUESTS:-10}
      - LLM_CLASSIFICATION_RATE_LIMIT_WINDOW=${LLM_CLASSIFICATION_RATE_LIMIT_WINDOW:-60}
      # LLM question extraction configuration (Phase 1.5 - Full LLM Solution)
      - ENABLE_LLM_EXTRACTION=${ENABLE_LLM_EXTRACTION:-false}
      - LLM_EXTRACTION_MODEL=${LLM_EXTRACTION_MODEL:-openai:gpt-4o-mini}
      - LLM_EXTRACTION_TEMPERATURE=${LLM_EXTRACTION_TEMPERATURE:-0.0}
      - LLM_EXTRACTION_MAX_TOKENS=${LLM_EXTRACTION_MAX_TOKENS:-4000}
      - LLM_EXTRACTION_BATCH_SIZE=${LLM_EXTRACTION_BATCH_SIZE:-10}
      - LLM_EXTRACTION_CACHE_TTL=${LLM_EXTRACTION_CACHE_TTL:-3600}
      - LLM_EXTRACTION_CACHE_SIZE=${LLM_EXTRACTION_CACHE_SIZE:-100}
      # Retriever backend configuration (Qdrant-only runtime)
      - RETRIEVER_BACKEND=${RETRIEVER_BACKEND:-qdrant}
      - HYBRID_SEMANTIC_WEIGHT=${HYBRID_SEMANTIC_WEIGHT:-0.6}
      - HYBRID_KEYWORD_WEIGHT=${HYBRID_KEYWORD_WEIGHT:-0.4}
      - COLBERT_TOP_N=${COLBERT_TOP_N:-5}
      - ENABLE_COLBERT_RERANK=${ENABLE_COLBERT_RERANK:-true}
      # Escalation pipeline feature flags
      - ESCALATION_ENABLED=${ESCALATION_ENABLED:-true}
      - ESCALATION_BISQ2_WS_ENABLED=${ESCALATION_BISQ2_WS_ENABLED:-false}
      - REACTION_NEGATIVE_STABILIZATION_SECONDS=${REACTION_NEGATIVE_STABILIZATION_SECONDS:-20}
    expose:
      - "8000"
    networks:
      - bisq-support-network
    healthcheck:
      # Use localhost for more reliable health checking during startup
      test: [ "CMD", "curl", "-fsS", "--max-time", "5", "http://localhost:8000/health" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 90s  # Extended to allow RAG service initialization (35-70s typical)
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    # Security hardening
    cap_drop:
      - ALL
    cap_add:
      - CHOWN        # Required for entrypoint to fix file ownership
      - DAC_OVERRIDE # Required for entrypoint to bypass permission checks
      - SETGID       # Required for gosu to switch group
      - SETUID       # Required for gosu to switch user
    security_opt:
      - no-new-privileges:true

  # Web frontend
  web:
    build:
      context: ..
      dockerfile: docker/web/Dockerfile
      args:
        - BUILD_ENV=production
        - NEXT_PUBLIC_API_URL=/api
    restart: unless-stopped
    expose:
      - "3000"
    environment:
      - NEXT_PUBLIC_API_URL=/api # Production-specific for Nginx routing
      - API_URL_INTERNAL=http://nginx:80 # SSR calls go through nginx for rate limiting and security headers
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1
    command: npm run start
    depends_on:
      api:
        condition: service_healthy
      # Note: nginx depends on web, so adding nginx here would create a circular dependency
      # SSR calls to nginx will simply fail during the brief startup window, which is acceptable
    networks:
      - bisq-support-network
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
    # Security hardening
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
      - /app/.next/cache

  # Qdrant vector database for hybrid search (semantic + BM25)
  qdrant:
    image: qdrant/qdrant:v1.12.1
    restart: unless-stopped
    volumes:
      - qdrant-data:/qdrant/storage
    expose:
      - "6333"
    networks:
      - bisq-support-network
    # No external ports - internal only for security
    # Note: qdrant/qdrant image doesn't include curl/wget
    # Use wget from busybox or check TCP connection as alternative
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c 'echo > /dev/tcp/localhost/6333' 2>/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  # Monitoring - Prometheus
  prometheus:
    image: prom/prometheus:latest
    restart: always
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus-data:/prometheus
      - ../runtime_secrets/prometheus_admin_key:/etc/prometheus/admin_key:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--storage.tsdb.retention.size=1GB'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    expose:
      - "9090"
    networks:
      - bisq-support-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G

  # Monitoring - Grafana
  grafana:
    image: grafana/grafana:latest
    restart: always
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-securepassword}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_INSTALL_PLUGINS=yesoreyeram-infinity-datasource
      - ADMIN_API_KEY_FOR_GRAFANA=${ADMIN_API_KEY}
      # Subpath configuration for nginx reverse proxy at /grafana/
      # Grafana handles subpath (receives /grafana/ prefix, serves from subpath)
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-%(protocol)s://%(domain)s/grafana/}
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      # Extended timeout for large HTML responses with inline JSON (~63KB)
      - GF_SERVER_REQUEST_TIMEOUT=120
    ports:
      - "${EXPOSE_GRAFANA_PORT:-3001}:3000"
    depends_on:
      - prometheus
    networks:
      - bisq-support-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # Node exporter for host metrics
  node-exporter:
    image: prom/node-exporter:latest
    restart: always
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
    expose:
      - "9100"
    networks:
      - bisq-support-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 256M

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.0
    restart: always
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    expose:
      - "8080"
    networks:
      - bisq-support-network
    privileged: true
    devices:
      - /dev/kmsg
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 512M

  # Alertmanager for alert routing and notification
  alertmanager:
    image: prom/alertmanager:v0.26.0
    restart: always
    volumes:
      - ./alertmanager:/etc/alertmanager
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    expose:
      - "9093"
    networks:
      - bisq-support-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 256M

  # NOTE: matrix-alertmanager-webhook service removed (Phase 9)
  # Alerts are now forwarded via API: /alertmanager/alerts
  # Uses password-based Matrix auth with automatic session refresh

  # Scheduler for periodic tasks
  scheduler:
    image: alpine:latest
    restart: always
    environment:
      - PROMETHEUS_URL=http://prometheus:9090
      - HEALTHCHECK_URL=${HEALTHCHECK_URL:-https://hc-ping.com/27867359-f4f3-4e22-8f46-8d18a73cf219}
      - ADMIN_API_KEY=${ADMIN_API_KEY}
      - API_HOST=api
    volumes:
      - ./scripts:/scripts:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./logs/cron:/var/log/cron
    command: >
      sh -c "
        apk add --no-cache docker-cli docker-compose bash coreutils curl jq bc &&
        mkdir -p /var/log/cron &&
        # Add @reboot entries to run scripts once when crond starts
        echo '@reboot /scripts/process-feedback.sh >> /var/log/cron/feedback.log 2>&1' > /etc/crontabs/root &&
        # Add regular cron entries
        # NOTE: update-faqs.sh removed - replaced by unified training pipeline (poll-matrix.sh)
        echo '0 1 * * 0 /scripts/update-wiki.sh >> /var/log/cron/wiki.log 2>&1' >> /etc/crontabs/root &&
        echo '0 0 * * 1 /scripts/process-feedback.sh >> /var/log/cron/feedback.log 2>&1' >> /etc/crontabs/root &&
        echo '*/15 * * * * /scripts/rag-health-check.sh >> /var/log/cron/rag-health.log 2>&1' >> /etc/crontabs/root &&
        echo '0 */12 * * * /scripts/poll-matrix.sh >> /var/log/cron/matrix-poll.log 2>&1' >> /etc/crontabs/root &&
        # Start cron daemon in foreground, logging verbosely to stderr (which goes to docker logs)
        crond -f -l 8 -d 8
      "
    depends_on:
      api:
        condition: service_healthy
      prometheus:
        condition: service_healthy
      alertmanager:
        condition: service_healthy
    networks:
      - bisq-support-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  prometheus-data:
  grafana-data:
  alertmanager-data:
  qdrant-data:
    name: bisq2-qdrant-data
  web_node_modules:
    name: bisq2-web-node-modules
  bisq2-data:
    name: bisq2-data
  feedback-data:
    name: bisq2-feedback-data
    driver: local

networks:
  bisq-support-network:
    driver: bridge
